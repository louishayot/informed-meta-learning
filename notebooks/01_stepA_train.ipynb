{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step A — Train baseline + alignment regimes\n",
    "\n",
    "Trains 3 regimes on the **set-trending-sinusoids** dataset with `abc2` knowledge:\n",
    "- `baseline` — no alignment\n",
    "- `aggressive_align_rT` — InfoNCE on posterior mean q(z|C,T)\n",
    "- `safe_align_rC` — InfoNCE on prior mean q(z|C)\n",
    "\n",
    "Each regime is trained for every seed in `SEEDS`.\n",
    "Outputs go to `outputs/{run_name}/` with `config.toml`, `metrics.jsonl`, `model_best.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/louishayot/MVA/VdS_Submission_Final\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "# Find repo root (parent of notebooks/)\n",
    "_nb_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "REPO_ROOT = os.path.abspath(os.path.join(_nb_dir, '..'))\n",
    "# Fallback: walk up until we find config.py\n",
    "_d = os.getcwd()\n",
    "while _d != os.path.dirname(_d):\n",
    "    if os.path.isfile(os.path.join(_d, 'config.py')):\n",
    "        REPO_ROOT = _d\n",
    "        break\n",
    "    _d = os.path.dirname(_d)\n",
    "\n",
    "os.chdir(REPO_ROOT)\n",
    "if REPO_ROOT not in sys.path:\n",
    "    sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "print(f'Working directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'outputs'\n",
    "FORCE_RETRAIN = False\n",
    "\n",
    "# ---- FAST_DEV: quick sanity check (1 seed, 50 epochs) ----\n",
    "FAST_DEV = False\n",
    "\n",
    "if FAST_DEV:\n",
    "    SEEDS = [0]\n",
    "    NUM_EPOCHS = 50\n",
    "else:\n",
    "    SEEDS = [0, 1, 2]\n",
    "    NUM_EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "\n",
    "def make_base_config(seed=0):\n",
    "    \"\"\"Base config matching run_sinusoids.sh INP_abc2 settings.\"\"\"\n",
    "    return Config(**{\n",
    "        # project\n",
    "        'project_name': 'INPs_sinusoids',\n",
    "        'seed': seed,\n",
    "        # training\n",
    "        'batch_size': 64,\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        'lr': 1e-3,\n",
    "        'beta': 1.0,\n",
    "        'sort_context': False,\n",
    "        'n_trials': 1,\n",
    "        'train_split': 'train',\n",
    "        'val_split': 'val',\n",
    "        'decay_lr': 10,\n",
    "        # dataset\n",
    "        'dataset': 'set-trending-sinusoids',\n",
    "        'knowledge_type': 'abc2',\n",
    "        'min_num_context': 0,\n",
    "        'max_num_context': 10,\n",
    "        'num_targets': 100,\n",
    "        'noise': 0.2,\n",
    "        'x_sampler': 'uniform',\n",
    "        # knowledge\n",
    "        'use_knowledge': True,\n",
    "        'text_encoder': 'set',\n",
    "        'freeze_llm': True,\n",
    "        'tune_llm_layer_norms': False,\n",
    "        'knowledge_dropout': 0.3,\n",
    "        'knowledge_merge': 'sum',\n",
    "        'knowledge_dim': 128,\n",
    "        'knowledge_extractor_num_hidden': 2,\n",
    "        'knowledge_extractor_hidden_dim': 128,\n",
    "        # model architecture\n",
    "        'input_dim': 1,\n",
    "        'output_dim': 1,\n",
    "        'hidden_dim': 128,\n",
    "        'x_transf_dim': 128,\n",
    "        'x_encoder_num_hidden': 1,\n",
    "        'xy_encoder_num_hidden': 2,\n",
    "        'xy_encoder_hidden_dim': 384,\n",
    "        'xy_self_attention': 'none',\n",
    "        'xy_self_attention_num_layers': 1,\n",
    "        'data_agg_func': 'mean',\n",
    "        'latent_encoder_num_hidden': 1,\n",
    "        'decoder_hidden_dim': 128,\n",
    "        'decoder_num_hidden': 3,\n",
    "        'decoder_activation': 'gelu',\n",
    "        'train_num_z_samples': 1,\n",
    "        'test_num_z_samples': 32,\n",
    "        'path': 'latent',\n",
    "        # alignment (overridden per regime)\n",
    "        'alignment_mode': 'none',\n",
    "        'alignment_lambda': 0.0,\n",
    "        'alignment_temperature': 0.1,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGIMES = {\n",
    "    'baseline': {\n",
    "        'alignment_mode': 'none',\n",
    "        'alignment_lambda': 0.0,\n",
    "    },\n",
    "    'aggressive_align_rT': {\n",
    "        'alignment_mode': 'rT',\n",
    "        'alignment_lambda': 0.1,\n",
    "        'alignment_temperature': 0.1,\n",
    "    },\n",
    "    'safe_align_rC': {\n",
    "        'alignment_mode': 'rC',\n",
    "        'alignment_lambda': 0.01,\n",
    "        'alignment_temperature': 0.2,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training: baseline_seed0\n",
      "============================================================\n",
      "Using device: cpu\n",
      "Trainable parameters:\n",
      "xy_encoder.pairer.layers.0.weight\n",
      "xy_encoder.pairer.layers.0.bias\n",
      "xy_encoder.pairer.layers.1.weight\n",
      "xy_encoder.pairer.layers.1.bias\n",
      "xy_encoder.pairer.layers.2.weight\n",
      "xy_encoder.pairer.layers.2.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.0.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.0.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.1.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.1.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.0.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.0.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.1.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.1.bias\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.0.weight\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.0.bias\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.1.weight\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.1.bias\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.2.weight\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.2.bias\n",
      "latent_encoder.encoder.layers.0.weight\n",
      "latent_encoder.encoder.layers.0.bias\n",
      "latent_encoder.encoder.layers.1.weight\n",
      "latent_encoder.encoder.layers.1.bias\n",
      "decoder.mlp.layers.0.weight\n",
      "decoder.mlp.layers.0.bias\n",
      "decoder.mlp.layers.1.weight\n",
      "decoder.mlp.layers.1.bias\n",
      "decoder.mlp.layers.2.weight\n",
      "decoder.mlp.layers.2.bias\n",
      "decoder.mlp.layers.3.weight\n",
      "decoder.mlp.layers.3.bias\n",
      "x_encoder.mlp.layers.0.weight\n",
      "x_encoder.mlp.layers.0.bias\n",
      "x_encoder.mlp.layers.1.weight\n",
      "x_encoder.mlp.layers.1.bias\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Best model saved at iteration 2000\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Best model saved at iteration 6000\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Best model saved at iteration 8500\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Best model saved at iteration 10000\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "  train_predictive_nll=-16.1946\n",
      "  eval_loss=53.9316\n",
      "  best_val_loss=17.0948  save_dir=outputs/baseline_seed0\n",
      "\n",
      "============================================================\n",
      "Training: baseline_seed1\n",
      "============================================================\n",
      "Using device: cpu\n",
      "Trainable parameters:\n",
      "xy_encoder.pairer.layers.0.weight\n",
      "xy_encoder.pairer.layers.0.bias\n",
      "xy_encoder.pairer.layers.1.weight\n",
      "xy_encoder.pairer.layers.1.bias\n",
      "xy_encoder.pairer.layers.2.weight\n",
      "xy_encoder.pairer.layers.2.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.0.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.0.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.1.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.1.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.0.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.0.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.1.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.1.bias\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.0.weight\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.0.bias\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.1.weight\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.1.bias\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.2.weight\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.2.bias\n",
      "latent_encoder.encoder.layers.0.weight\n",
      "latent_encoder.encoder.layers.0.bias\n",
      "latent_encoder.encoder.layers.1.weight\n",
      "latent_encoder.encoder.layers.1.bias\n",
      "decoder.mlp.layers.0.weight\n",
      "decoder.mlp.layers.0.bias\n",
      "decoder.mlp.layers.1.weight\n",
      "decoder.mlp.layers.1.bias\n",
      "decoder.mlp.layers.2.weight\n",
      "decoder.mlp.layers.2.bias\n",
      "decoder.mlp.layers.3.weight\n",
      "decoder.mlp.layers.3.bias\n",
      "x_encoder.mlp.layers.0.weight\n",
      "x_encoder.mlp.layers.0.bias\n",
      "x_encoder.mlp.layers.1.weight\n",
      "x_encoder.mlp.layers.1.bias\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Best model saved at iteration 2000\n",
      "Evaluating\n",
      "Best model saved at iteration 2500\n",
      "Evaluating\n",
      "Best model saved at iteration 3000\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Best model saved at iteration 5500\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Best model saved at iteration 8000\n",
      "Evaluating\n",
      "Best model saved at iteration 8500\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Best model saved at iteration 11500\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Best model saved at iteration 15000\n",
      "Evaluating\n",
      "Evaluating\n",
      "  train_predictive_nll=-14.6175\n",
      "  eval_loss=52.1185\n",
      "  best_val_loss=20.7324  save_dir=outputs/baseline_seed1\n",
      "\n",
      "============================================================\n",
      "Training: baseline_seed2\n",
      "============================================================\n",
      "Using device: cpu\n",
      "Trainable parameters:\n",
      "xy_encoder.pairer.layers.0.weight\n",
      "xy_encoder.pairer.layers.0.bias\n",
      "xy_encoder.pairer.layers.1.weight\n",
      "xy_encoder.pairer.layers.1.bias\n",
      "xy_encoder.pairer.layers.2.weight\n",
      "xy_encoder.pairer.layers.2.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.0.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.0.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.1.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.1.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.0.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.0.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.1.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.1.bias\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.0.weight\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.0.bias\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.1.weight\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.1.bias\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.2.weight\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.2.bias\n",
      "latent_encoder.encoder.layers.0.weight\n",
      "latent_encoder.encoder.layers.0.bias\n",
      "latent_encoder.encoder.layers.1.weight\n",
      "latent_encoder.encoder.layers.1.bias\n",
      "decoder.mlp.layers.0.weight\n",
      "decoder.mlp.layers.0.bias\n",
      "decoder.mlp.layers.1.weight\n",
      "decoder.mlp.layers.1.bias\n",
      "decoder.mlp.layers.2.weight\n",
      "decoder.mlp.layers.2.bias\n",
      "decoder.mlp.layers.3.weight\n",
      "decoder.mlp.layers.3.bias\n",
      "x_encoder.mlp.layers.0.weight\n",
      "x_encoder.mlp.layers.0.bias\n",
      "x_encoder.mlp.layers.1.weight\n",
      "x_encoder.mlp.layers.1.bias\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Best model saved at iteration 2000\n",
      "Evaluating\n",
      "Evaluating\n",
      "Best model saved at iteration 3000\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Best model saved at iteration 5000\n",
      "Evaluating\n",
      "Best model saved at iteration 5500\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Best model saved at iteration 14000\n",
      "Evaluating\n",
      "Best model saved at iteration 14500\n",
      "Evaluating\n",
      "Best model saved at iteration 15000\n",
      "Evaluating\n",
      "Evaluating\n",
      "  train_predictive_nll=-17.5014\n",
      "  eval_loss=66.5221\n",
      "  best_val_loss=19.6076  save_dir=outputs/baseline_seed2\n",
      "\n",
      "============================================================\n",
      "Training: aggressive_align_rT_seed0\n",
      "============================================================\n",
      "Using device: cpu\n",
      "Trainable parameters:\n",
      "xy_encoder.pairer.layers.0.weight\n",
      "xy_encoder.pairer.layers.0.bias\n",
      "xy_encoder.pairer.layers.1.weight\n",
      "xy_encoder.pairer.layers.1.bias\n",
      "xy_encoder.pairer.layers.2.weight\n",
      "xy_encoder.pairer.layers.2.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.0.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.0.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.1.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.1.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.0.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.0.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.1.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.1.bias\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.0.weight\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.0.bias\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.1.weight\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.1.bias\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.2.weight\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.2.bias\n",
      "latent_encoder.encoder.layers.0.weight\n",
      "latent_encoder.encoder.layers.0.bias\n",
      "latent_encoder.encoder.layers.1.weight\n",
      "latent_encoder.encoder.layers.1.bias\n",
      "decoder.mlp.layers.0.weight\n",
      "decoder.mlp.layers.0.bias\n",
      "decoder.mlp.layers.1.weight\n",
      "decoder.mlp.layers.1.bias\n",
      "decoder.mlp.layers.2.weight\n",
      "decoder.mlp.layers.2.bias\n",
      "decoder.mlp.layers.3.weight\n",
      "decoder.mlp.layers.3.bias\n",
      "x_encoder.mlp.layers.0.weight\n",
      "x_encoder.mlp.layers.0.bias\n",
      "x_encoder.mlp.layers.1.weight\n",
      "x_encoder.mlp.layers.1.bias\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Best model saved at iteration 2000\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Best model saved at iteration 6000\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Best model saved at iteration 8500\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Best model saved at iteration 10000\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "  train_predictive_nll=-16.8580\n",
      "  eval_loss=53.8810\n",
      "  best_val_loss=19.9123  save_dir=outputs/aggressive_align_rT_seed0\n",
      "\n",
      "============================================================\n",
      "Training: aggressive_align_rT_seed1\n",
      "============================================================\n",
      "Using device: cpu\n",
      "Trainable parameters:\n",
      "xy_encoder.pairer.layers.0.weight\n",
      "xy_encoder.pairer.layers.0.bias\n",
      "xy_encoder.pairer.layers.1.weight\n",
      "xy_encoder.pairer.layers.1.bias\n",
      "xy_encoder.pairer.layers.2.weight\n",
      "xy_encoder.pairer.layers.2.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.0.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.0.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.1.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h1.layers.1.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.0.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.0.bias\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.1.weight\n",
      "latent_encoder.knowledge_encoder.text_encoder.h2.layers.1.bias\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.0.weight\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.0.bias\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.1.weight\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.1.bias\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.2.weight\n",
      "latent_encoder.knowledge_encoder.knowledge_extractor.layers.2.bias\n",
      "latent_encoder.encoder.layers.0.weight\n",
      "latent_encoder.encoder.layers.0.bias\n",
      "latent_encoder.encoder.layers.1.weight\n",
      "latent_encoder.encoder.layers.1.bias\n",
      "decoder.mlp.layers.0.weight\n",
      "decoder.mlp.layers.0.bias\n",
      "decoder.mlp.layers.1.weight\n",
      "decoder.mlp.layers.1.bias\n",
      "decoder.mlp.layers.2.weight\n",
      "decoder.mlp.layers.2.bias\n",
      "decoder.mlp.layers.3.weight\n",
      "decoder.mlp.layers.3.bias\n",
      "x_encoder.mlp.layers.0.weight\n",
      "x_encoder.mlp.layers.0.bias\n",
      "x_encoder.mlp.layers.1.weight\n",
      "x_encoder.mlp.layers.1.bias\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Best model saved at iteration 2000\n",
      "Evaluating\n",
      "Best model saved at iteration 2500\n",
      "Evaluating\n",
      "Best model saved at iteration 3000\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n",
      "Evaluating\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from models.train import train_from_config\n",
    "\n",
    "for regime_name, overrides in REGIMES.items():\n",
    "    for seed in SEEDS:\n",
    "        run_name = f'{regime_name}_seed{seed}'\n",
    "        ckpt_path = os.path.join(OUTPUT_DIR, run_name, 'model_best.pt')\n",
    "\n",
    "        if os.path.exists(ckpt_path) and not FORCE_RETRAIN:\n",
    "            print(f'SKIP {run_name} (checkpoint exists)')\n",
    "            continue\n",
    "\n",
    "        print(f'\\n{\"=\"*60}')\n",
    "        print(f'Training: {run_name}')\n",
    "        print(f'{\"=\"*60}')\n",
    "\n",
    "        config = make_base_config(seed=seed)\n",
    "        for k, v in overrides.items():\n",
    "            setattr(config, k, v)\n",
    "\n",
    "        best_loss, save_dir = train_from_config(\n",
    "            config, output_dir=OUTPUT_DIR, run_name=run_name, use_wandb=False\n",
    "        )\n",
    "\n",
    "        # Print summary from last metrics line\n",
    "        metrics_path = os.path.join(save_dir, 'metrics.jsonl')\n",
    "        if os.path.exists(metrics_path):\n",
    "            with open(metrics_path) as f:\n",
    "                lines = f.readlines()\n",
    "            last_train = None\n",
    "            last_eval = None\n",
    "            for line in reversed(lines):\n",
    "                d = json.loads(line)\n",
    "                if last_eval is None and 'eval_loss' in d:\n",
    "                    last_eval = d\n",
    "                if last_train is None and 'train_predictive_nll' in d:\n",
    "                    last_train = d\n",
    "                if last_train and last_eval:\n",
    "                    break\n",
    "            if last_train:\n",
    "                msg = f'  train_predictive_nll={last_train[\"train_predictive_nll\"]:.4f}'\n",
    "                if 'train_alignment_loss' in last_train:\n",
    "                    msg += f'  align_loss={last_train[\"train_alignment_loss\"]:.4f}'\n",
    "                print(msg)\n",
    "            if last_eval:\n",
    "                print(f'  eval_loss={last_eval[\"eval_loss\"]:.4f}')\n",
    "\n",
    "        print(f'  best_val_loss={best_loss:.4f}  save_dir={save_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run outputs:\n",
      "  baseline_seed0                            ckpt=False  metrics=False  config=False  [INCOMPLETE]\n",
      "  baseline_seed1                            ckpt=False  metrics=False  config=False  [INCOMPLETE]\n",
      "  baseline_seed2                            ckpt=False  metrics=False  config=False  [INCOMPLETE]\n",
      "  aggressive_align_rT_seed0                 ckpt=False  metrics=False  config=False  [INCOMPLETE]\n",
      "  aggressive_align_rT_seed1                 ckpt=False  metrics=False  config=False  [INCOMPLETE]\n",
      "  aggressive_align_rT_seed2                 ckpt=False  metrics=False  config=False  [INCOMPLETE]\n",
      "  safe_align_rC_seed0                       ckpt=False  metrics=False  config=False  [INCOMPLETE]\n",
      "  safe_align_rC_seed1                       ckpt=False  metrics=False  config=False  [INCOMPLETE]\n",
      "  safe_align_rC_seed2                       ckpt=False  metrics=False  config=False  [INCOMPLETE]\n"
     ]
    }
   ],
   "source": [
    "# Verify outputs\n",
    "print('Run outputs:')\n",
    "for regime_name in REGIMES:\n",
    "    for seed in SEEDS:\n",
    "        run_name = f'{regime_name}_seed{seed}'\n",
    "        run_dir = os.path.join(OUTPUT_DIR, run_name)\n",
    "        has_ckpt = os.path.exists(os.path.join(run_dir, 'model_best.pt'))\n",
    "        has_metrics = os.path.exists(os.path.join(run_dir, 'metrics.jsonl'))\n",
    "        has_config = os.path.exists(os.path.join(run_dir, 'config.toml'))\n",
    "        status = 'OK' if (has_ckpt and has_metrics and has_config) else 'INCOMPLETE'\n",
    "        print(f'  {run_name:40s}  ckpt={has_ckpt}  metrics={has_metrics}  config={has_config}  [{status}]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv-radar)",
   "language": "python",
   "name": "venv-radar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
